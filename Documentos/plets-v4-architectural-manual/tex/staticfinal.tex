\chapter{Compilation Directives}
\label{ch:staticfinal}

In order to control variability, it is important to have mechanisms in place to manage that variability in terms of configuration (both in terms of products and in terms of modules), traceability (identifying portions of code and linking them to models), solution building (again, both in terms of products and in terms of modules), access security (management of code-access permissions), non-functional properties (managing variability in quality terms such as performance and reliability) and several other concerns that exist in the fields of Software Engineering that deal with variability, such as \acrfull{cbse} and \acrfull{sple}.

Though variability mechanisms can be considered to have a ``\gls{bindtime}'' (such as compile, link, run, load, etc.), this chapter considers only the variability mechanisms for Compile-Time variation. This focus is due to our current focus being on \emph{static} \acrshort{spl}, which are \acrshort{spl} where variation is handled during the configuration of a product, rather than while the product is running.

Compilation directives are well-known in C++, often identified as ``\#ifdef''. While C\# made a similar mechanism available to us (\#if), Java is lacking in an in-built preprocessor, and therefore has no mechanism with the exact same purpose as the C++ compilation directives. This chapter presents a short introduction to compilers and the concept of compilation directives and preprocessors, as well as the reasons why these are used in PLeTs and how to use Static Final variables in Java to reach a similar result.

%----------------------------------------------------------

\section*{How to read this chapter}

This chapter is very theory-heavy, and therefore may be a ``heavier'' read for people less acquainted with the concepts presented. Furthermore, the depth at which they are presented is not meant to fully clarify their inner workings, as there are entire books dedicated to that \cite{AHO:2006} \cite{CZARNECKI:2000}.

Section \ref{sc:comp} covers the basic concepts of compiler technology, relating the notions explained to their use in variability management and implementing compiler directives. It is recommended that this section be read more than once, as understanding the concepts explained at the end of the section may clarify some of the concepts presented at the beginning, and vice-versa.

Section \ref{sc:compjava} more specifically analyses the means of using compilation directives in the Java language. This section covers the in-built mechanisms that Java has available and why they are insufficient to our purposes. Understanding this section is important for the appropriate use of directives, and crucial to the developers in charge of the Java Preprocessor project.

The Java \gls{grammar}s and \gls{regex}s are purely pedagogical, and in no way represent the real syntax of a Java compiler. For the official specification of the Java language, see \cite{GOSLING:2014}. Knowledge of the Java specification is \emph{not} required for this project, and is here for referencing purposes only.

%----------------------------------------------------------

\section{Compilers}
\label{sc:comp}

Most of our professionals are unaware of the exact way through which compilers work, but a basic understanding of their mechanisms is necessary to fully comprehend the uses of compilation directives. Compilers can generally be divided in three or four smaller tools that, together, execute the entire compilation process. These are the Lexical Analyser, the Syntactic Analyser, the Semantic Analyser, and, optionally, the Code Generator. Other features can be included in compilers, such as code optimizers, but these are not covered here.

Section \ref{sc:regex} covers the basics of \gls{regex}s, which are fundamental tools for the appropriate implementation of almost any software system. Section \ref{sc:lex} briefly explains the purpose of lexical analysers, and how they make use of \gls{regex}s to prepare a stream of text for compilation. Section \ref{sc:syn} attempts to give a sufficient understanding of syntactic analysis and \gls{grammar}s within their application in compiler technology. Finally, Section \ref{sc:sem} presents the basics of semantic analysis, in order to prepare the reader for the more advanced concepts presented in Section \ref{sc:compjava}.

%----------------------------------------------------------

\subsection{Regular Expressions}
\label{sc:regex}

The simplest unit of analysis that a compiler uses is a stream of text, or a string. These streams of text, while usually structured according to a \gls{grammar}, may be formed of any number of compositions of characters, and identifying these compositions as the structural units of a \gls{grammar} require these units to have their own individual composition rules. These rules are called \gls{regex}s. A \gls{regex} is a structured string of characters that define a pattern. This pattern can then be compared to any other string to see whether it conforms to the \gls{regex} or not.

We will use as an example the typical ``Hello World'' Java program, based on the code presented in Listing \ref{lst:helloworldjava}, taken from \cite{SEDGEWICK:2011}.

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
public class HelloWorld {
  public static void main(String[] args) {
      // Prints "Hello, World" to the terminal window.
      System.out.println("Hello, World");
     }
}
\end{minted}
\caption{HelloWorld.java from \cite{SEDGEWICK:2011}} \label{lst:helloworldjava}
\end{listing}

This code is what is called a \emph{valid word} within the Java language, meaning it conforms to the language's \gls{grammar}. However, in order for a syntactical analyser to be able to tell that this code is a valid word, it needs to know how the code, or stream of text, is \emph{structured}. \Gls{regex}s are the first step towards that goal. Let us take, for example, the \gls{regex}s in Listing \ref{lst:javaregex}, which is a (very) simplified example of some of the \gls{regex}s implemented in any Java compiler.

\begin{listing}
\begin{grammar}
<ACCESS_MODIFIER> ::= public | private | protected

<CLASS> ::= class

<IDENTIFIER> ::= [A-Za-z_\$][A-Za-z_\$0-9]*

<LEFT_BRACKET> ::= \{

<SPACE> ::= [ \textbackslash r\textbackslash n\textbackslash t]+
\end{grammar}
\caption{Reduced example of possible \gls{regex}s for Java tokens.} \label{lst:javaregex}
\end{listing}

If we follow the rules set out in Listing \ref{lst:javaregex} to analyse the code in Listing \ref{lst:helloworldjava}, we will find that the first item in the stream conforms to the rule ``ACCESS_MODIFIER'' (public), the second conforms to ``SPACE'' (the empty space between ``public'' and ``class''), the third to ``CLASS'' (class) and so on. \Gls{regex}s like ``ACCESS_MODIFIER'' and ``CLASS'', where the expected string is an entire word rather than a pattern of characters, are typically used for identifying \emph{reserved words}, words that have special meaning in a language. The more interesting type of \gls{regex}s however are the ones like ``IDENTIFIER'' and ``SPACE'', where a pattern is expected rather than a word.

\Gls{regex} patterns vary from language to language, but they generally look alike: characters inside brackets ([ ]) form a set, where any of the characters inside the set is acceptable in that position; escape characters like ``\textbackslash r'' and ``\textbackslash n'' are counted as a single character, since they are represented by a single ASCII byte; the * symbol means that anything that comes before it, usually a set, may be matched zero or more times, and; the + character means that anything that comes before it, usually a set, may be matched one or more times.

From these definitions, we may read that an ``IDENTIFIER'' is any string formed by a letter, underscore or \$, followed by any number of letters, underscores, \$ or digits. Likewise, a ``SPACE'' is any string formed by one or more space, \textbackslash r, \textbackslash n or \textbackslash t. By using \gls{regex}s like these it is possible to create patterns to represent any sequence of characters (the website RegexOne \cite{REGEXONE} is an excellent source of exercises for learning the basics of \gls{regex}s).

It is important to note that when a single string matches more than one \gls{regex}, the decision of which to use is based on the priority of the rules within the ruleset and on which rule is ``best matched'', that is, which rule matches the string with the most units used. For example, the string ``public'' could very well match the ``IDENTIFIER'' rule, but the ``ACCESS_MODIFIER'' rule has both priority over ``IDENTIFIER'' (reserved word rules almost always take priority) and  provides a better match, since ``public'' matches six units of the ``ACCESS_MODIFIER'' but only two units of ``IDENTIFIER'' (each set counts as one unit).

Understanding and being able to write \gls{regex}s is a basic skill that is expected of any programmer; it is extremely recommended that all members of the project understand this section and do the exercises on RegexOne. More than being used in compiler technology, \gls{regex}s are a basic building block in any system that performs analysis of strings, validation of fields and a large number of other procedures.

%----------------------------------------------------------

\subsection{Lexical Analyser}
\label{sc:lex}

The lexical analyser is the part of a compiler that is responsible for \emph{tokenizing} the input text. What this means is that the stream that is sent as input to the compiler is fully read and divided into its basic units, such as ``identifiers'' and ``reserved words'', by matching them to a list of \gls{regex}s that define each type of token. Essentially, a stream of text is transformed into a list of values that the compiler understands. This is important for the compilation process as the steps that follow (syntactic and semantic analysis) \emph{are not entirely concerned with the exact text included in the input, but rather with its structure}. This tokenization process is generally executed in a single pass (meaning the stream is read only once), and outputs an ordered list of tokens. To better understand the process of lexical analysis, let us take another look at Listing \ref{lst:helloworldjava}.

In analyzing the ``HelloWorld'' code, which is read as a single stream of text (one large string), a lexical analyser may identify tokens like ``ACCESS_MODIFIER'' (public), ``CLASS'' (class), ``IDENTIFIER'' (HelloWorld), ``LEFT_BRACKET'' (\{), ``ACCESS_MODIFIER'' (public), and so on based on the \gls{regex} that define its tokens. Notice that the exact contents of a token are irrelevant at this point in the analysis; rather, the lexical analyser identifies the \emph{type} of unit that a particular piece of code represents. This is because the exact text is irrelevant to the correct syntax of the code; as we know, we can name a Java class pretty much anything, so long as it follows a simple \gls{regex} like the ``IDENTIFIER'' rule in Listing \ref{lst:javaregex}. Whether the name of the class is ``HelloWorld'' or ``ClassNameHere'', the form of the code will remain the same.

Also notice that blank spaces, such as the ones defined by the rule ``SPACE'' in Listing \ref{lst:javaregex} are generally not tokenized. This is because the blank space is usually used as a wildcard for the delimitation between two tokens, which means the number of blank spaces is not important to the tokenization, and also means that the simple fact of the tokenizer separating, say, ``ACCESS_MODIFIER'' and ``CLASS'' already acknowledges that there was indeed a space between them (the space is implicit in the tokenization). This is not always the case, however, as languages like Python do use indentation in their syntax.

The importance of understanding lexical analysis for the understanding of conditional compilation is that the conditional statements, such as \#ifdef or \#if, or the constant declaration \#define, are in fact tokens to the compiler. Whether the token value expected by the compiler is \#ifdef, \#if, or something else entirely (say, \#condition, for example) is not important: it is the meaning of these tokens that matters, and the lexical analyser may very well group them all as a ``CONDITIONAL_CLAUSE'' token class. For this reason, despite there being no specific preprocessor in Java, it is plausible to imagine a system in Java where a conditional compilation clause is created in the form of a token class ``CONDITIONAL_CLAUSE'' or anything of the sort, where the value is whatever we choose for it. Listing \ref{lst:javaregex2} shows a possible rule to represent this concept.

\begin{listing}
\begin{grammar}
<CONDITIONAL_CLAUSE> ::= \#ifdef | \#if | \#condition
\end{grammar}
\caption{Example of conditional token class.} \label{lst:javaregex2}
\end{listing}

Note that the use of the escape character \# before each of the possible values of ``CONDITIONAL_CLAUSE'' means the lexical analyser will be able to skip almost all of the text stream, whereas the uses of the same strings without an escape character would mean the lexical analyser would need to validate every string that partially matched the rules, such as ``conformity'' (matches ``con''). This is done both in order to speed up the process and to limit the reserved words in a language. The fewer reserved words, the more freedom a programmer has to name variables and functions, and therefore make code more readable.

Understanding lexical analysis on its own may appear confusing, as it is largely just a tool to be used by the syntactic analyser. For this reason, feel free to return to this section after you understand syntactic analysis, as it may make more sense to you then.

%----------------------------------------------------------

\subsection{Syntactic Analyser}
\label{sc:syn}

The syntactic analyser is charged with making sense of the tokens provided by the lexical analyser. What it does is check the order of these tokens against a set of rules, called a \gls{grammar}, to see whether the input text makes sense or not (whether it is a \emph{valid word} of the language). The importance of this is to guarantee that the input text is in the right format for the compiler, so that it may be able to interpret the file properly. Think of it as checking the structure of a sentence in natural language, and making sure that pronouns, nouns, verbs, adjectives, etc. are used correctly.

Listing \ref{lst:simpgrammar} presents a simplified grammar based on the code from Listing \ref{lst:helloworldjava} and the \gls{regex}s from Listing \ref{lst:javaregex}. Note that the \gls{grammar} is presented in what is known as the \acrfull{bnf}, the format usually used to represent a \emph{context-free grammar}. Also note that typically, ``terminal rules'' (rules that translate to a lexical token) are typically written in capitals, where ``non-terminal rules'' (rules that translate into further rules) are typically written in non-capitals.

\begin{listing}
\begin{grammar}
<class> ::= <ACCESS_MODIFIER> <CLASS> <IDENTIFIER> <LEFT_BRACKET> <class_inside> <RIGHT_BRACKET>

<class_inside> ::= <comments> <declaration>

<comments> ::= $\epsilon$ | ...

<declaration> ::= <method> | <attribute> | <declaration> <declaration> | $\epsilon$
\end{grammar}
\caption{Reduced example of a possible grammar to identify a Java class code} \label{lst:simpgrammar}
\end{listing}

This \gls{grammar} specifies that a class is comprised of an access modifier (public, private, protected, etc.), the reserved word ``class'', an identifier (like ``HelloWorld'' or ``ClassNameHere''), a left bracket (\{), the contents of the class (methods, attributes, etc.), and a right bracket (\}). The contents of the class (``class_inside''), in turn, are formed of comments and declarations. Comments can either be empty (represented by $\epsilon$) or made up of a group of rules specific for formatting comments (omitted here). Declarations can either be empty, a method or an attribute; in this special case, the sequence \textless declaration\textgreater \ \textless declaration\textgreater \ is used to demonstrate that a declaration may be formed of two declarations. Essentially, this means that there can be any number of declarations in a class; it is akin to the + symbol in \gls{regex}s.

Effectively, what the syntactic analyser will do is take the tokens given it by the tokenizer and check them against the \gls{grammar}. It knows that the base rule is \textless class\textgreater \ (the first rule in the grammar), so the first thing it looks for is a token ``ACCESS_MODIFIER''. It goes from there until it reaches the rule \textless class_inside\textgreater , which causes it to shift what it is looking for from the \textless class\textgreater \ rule to the \textless class_inside\textgreater \ rule.

Let us consider a more practical example towards the problem our project is faced with: conditional compilation.

\begin{listing}
\begin{grammar}
<conditional> ::= <CONDITIONAL_CLAUSE> <IDENTIFIER> <code> <END_CONDITIONAL_CLAUSE>

<code> ::= ...
\end{grammar}
\caption{Example of non-terminal conditional rule.} \label{lst:condgrammar}
\end{listing}

The rules in Listing \ref{lst:condgrammar} define that if the syntactic analyser finds a ``CONDITIONAL_CLAUSE'' token, it must verify whether the next token is an ``IDENTIFIER'', and so on. Adapting the HelloWorld example based on Listings \ref{lst:javaregex2} and \ref{lst:condgrammar}, we could create a code that may be evaluated by the ``conditional'' rule like the one in Listing \ref{lst:helloworldjavacond}.

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
#define boolean PRINT = true;

public class HelloWorld {
  public static void main(String[] args) {
      #if PRINT
      // Prints "Hello, World" to the terminal window.
      System.out.println("Hello, World");
      #endif
     }
}
\end{minted}
\caption{Adapted from HelloWorld.java from \cite{SEDGEWICK:2011}} \label{lst:helloworldjavacond}
\end{listing}

The lexical analyser will have read ``\#if'' as CONDITIONAL_SYMBOL, ``PRINT'' as IDENTIFIER and ``\#endif'' as END_CONDITIONAL_SYMBOL, with everything in-between being analysed as normal code (any number of other tokens). The syntactic analyser will then be able to realize that the code in that section falls in a conditional compilation rule, and will set it aside for special treatment by the semantic analyser or code generator. In practice, we have established that our HelloWorld class will only print ``Hello, World'' on the screen if we define the constant PRINT when compiling.

At this point, you should understand the basics of why we define tokens, and how they are interpreted by the syntactic analyser. However, this alone is not enough to fully give meaning to an input stream; we have decided that our code is a valid word in our language, but we do not yet know what that word \emph{means}. That is the job of the semantic analyser.

%----------------------------------------------------------

\subsection{Semantic Analyser}
\label{sc:sem}

The semantic analyser is in charge of deciding what the input text means after the syntactic analyser has assured it that the text is well-formed. The semantic analyser will essentially be a software application itself, treating input text according to what it receives. For example, a semantic analyser for the previous example in Listing \ref{lst:condgrammar} would include a program, or a piece of code, stating that when the syntactic rule ``conditional'' is found, the string identified by the rule ``code'' will only be compiled if the constant named by the ``IDENTIFIER'' token was defined before compilation. In the case of the code in Listing \ref{lst:helloworldjavacond}, the semantic analyser would state that if PRINT is not defined, then the statement inside the conditional should not be compiled. Essentially, it says ``if PRINT is false, ignore everything in this block''.

This is where we meet our problem with the Java compiler, and where our Java Preprocessor study comes in. To be able to identify these kinds of semantic rules, C++ and C\# (for example) possess what is called a preprocessor. What this preprocessor does is take in a number of constant definitions, like ``int TWO = 2'' or ``boolean PRINT = true'', and replace every instance of that constant in the code with the value of the constant. Essentially, after the preprocessor has done its work, Listing \ref{lst:helloworldjavacond} would look like this:

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
public class HelloWorld {
  public static void main(String[] args) {
      #if true
      // Prints "Hello, World" to the terminal window.
      System.out.println("Hello, World");
      #endif
     }
}
\end{minted}
\caption{Adapted from HelloWorld.java from \cite{SEDGEWICK:2011}} \label{lst:helloworldjavaconditional2}
\end{listing}

As you can see, the constant definition is removed entirely from the program (since it has already served its purpose) and all of its appearances were replaced with its value. By doing this, the preprocessor allows the compiler to know that section of code should be compiled; if PRINT had been equal to false (that is, the compilation constant PRINT was not defined), then the compiler would know to ignore that section of code.

What happens in Java is that without a preprocessor we have no means of telling the compiler which parts of the code should or should not be compiled; we cannot define constants to be read in that way. In fact, the preprocessor for C\# already knows what \#if and \#endif mean, and if PRINT had been false, the preprocessor itself would have removed that code. The compiler would never even know it existed. The reason why this is done by a preprocessor, rather than the normal compiler, is two-fold. First, it allows the designers of the language to create two separate \gls{grammar}s, one for the preprocessor and one for the compiler, where both are much simpler and easier to understand than they would be if they were a single thing. Second, it reduces the number of times that the compiler must read the code before creating the executable (generating assembly code), making it much faster.

The Java Preprocessor study is therefore dedicated to finding a way to define conditional compilation in Java, so that portions of code may be compiled or not based on the configuration of a product within an \acrshort{spl}. Ultimately, by creating a program that takes in the Java source code and transforms it, we are able to tell the compiler exactly what we want, and what we don't want in our executables (.class and .jar files).

%----------------------------------------------------------

\section{Compilation Directives in Java}
\label{sc:compjava}

In order to implement a system that has \gls{variability}, mechanisms must be set in place in the code to control it, trace it and maintain it. The number of proposed \gls{variability} control mechanisms is quite possibly uncountable, with new ones being proposed every year to handle novel situations in \acrshort{sple}, \acrshort{cbse} and other software engineering fields that rely on \gls{variability}. In this section, we will explore \gls{variability} control mechanisms specific to the Java programming language. All of the mechanisms presented in this section have merit, and each is suited for different kinds of \gls{variability} management. Therefore, a good understanding of all mechanisms and when to apply them is important.

Section \ref{sc:sfa} presents a ``quick-fix'' form of \gls{variability} management in Java, relying on the built-in code optimizers of Java compilers to force \gls{variability} into the source code. Section \ref{sc:reflection} explores the possibility of using Java code reflection to implement \gls{variability} points. Section \ref{sc:interfaces} re-evaluates the primary mechanism of PLeTs v1, interfaces, as well as presenting a new form of using them for selecting \gls{variant}s. Finally, Section \ref{sc:prepros} presents the proposal of a Java preprocessor to include first-class compilation directives into the language.

%----------------------------------------------------------

\subsection{Static Final Attributes}
\label{sc:sfa}

One way to get around the problem of Java not having a preprocessor is by abusing the optimization features that Java compilers do generally have. In the standard Java compilers, any code that is unreachable is automatically removed by the compiler's optimizer, which means that we are able to remove certain parts of code by making sure they can never be executed. Essentially, we create conditions of the form \lstinline{if(true)} and \lstinline{if(false)} to encapsulate our \gls{variant} code.

To do this, one can use what is called a \lstinline{Static Final} attribute. When a conditional loop evaluates itself with a single boolean attribute, and that attribute is always false, that loop will be removed by the optimizer due to being unreachable. Usually the compiler cannot know if the condition will always be false or not, given that this would require a complex analysis of a state machine, but since a \lstinline{Static Final} attribute can never change its value, the optimizer will be able to tell. Listings \ref{lst:preprocessor} and \ref{lst:staticfinal} show an example of how this can be used.

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
public class Preprocessor
{
 public static final boolean FULLACCESS = false;
 ...
}
\end{minted}
\caption{Preprocessor.java, encapsulating variability points} \label{lst:preprocessor}
\end{listing}

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
import Preprocessor;

public class UmlAssociation extends UmlElement implements Serializable
{
...
  private UmlElement end1Element;
...
  public UmlElement getEnd1()
  {
    if(Preprocessor.FULLACCESS)
    {
      return this.end1Element;
    }
    else
    {
      return DeepCopy.copy(this.end1Element);
    }
  }
...
}
\end{minted}
\caption{Variability point being resolved by the Preprocessor.java attributes, taken from \cite{AUTOREST}} \label{lst:staticfinal}
\end{listing}

In the example, a Preprocessor class is declared in Listing \ref{lst:preprocessor} which contains all of the variability point constants for free access from the rest of the project's packages; essentially, we are centralizing the variability management in a single artifact. The packages may then use these attributes in conventional conditional loops to transform them into conditional compilation loops. In Listing \ref{lst:staticfinal}, the method getEnd1 is conditionally compiled so that if the selected product has full access (a \gls{variability} point of the AutoREST application \cite{AUTOREST} that represents security access level), a reference to end1Element is returned. Meanwhile, if the selected product does not have full access, only a copy is returned, ensuring that the object cannot be modified from outside. This ensures that if the product does not have full access, the user will not be able to directly alter the end1Element variable, being forced to use other methods to do so.

This approach has one very important limitation: it can only be used where normal conditional loops may be used. Therefore, it is impossible to conditionally compile an entire method or class, as Java does not permit conditional loop statements to be outside method bodies (remember \gls{grammar}s: the rule for \emph{if} statements would be inside the rule for \emph{methods}, meaning that the syntactic analyser would only accept the token ``IF'' when it is found inside a method). While it works in the example above, it does work in the example presented in Listing \ref{lst:staticfinalerror}.

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
import Preprocessor;

public class UmlAssociation extends UmlElement implements Serializable
{
...
  //INCORRECT JAVA SYNTAX, DOES NOT COMPILE!!!!!
  if(FULLACCESS)
  {
    public UmlAssociation()
    {
      BuildIt(null, null, null, null, null, null, null, null, null);
    }
  }
...
}
\end{minted}
\caption{Limitations of Static Final attributes, taken from \cite{AUTOREST}} \label{lst:staticfinalerror}
\end{listing}

In this example, an entire constructor should be conditionally compiled so that this particular overload is only available if the configured product has full access. While it is possible to place the method's inner code inside a conditional loop, that would mean this overload would still exist, and a programmer using an IDE like NetBeans or Eclipse would be fooled into thinking it is available, when in fact it is not. The ideal way of solving this problem would be to remove the method entirely from the compiled .jar file, so that the programmer is not tricked into thinking the method exists.

The \lstinline{Static Final} form of \gls{variability} management is perfectly valid for situations where a single method may have two different implementations depending on product configuration. It is a simpler way of controlling \gls{variability} than the other approaches discussed ahead, but it must be used with caution. An especially dangerous effect of using \lstinline{Static Final} attributes for \gls{variability} management is that the documentation (Javadoc) cannot be conditionally compiled using this approach, making it virtually impossible to document a variable method properly.

In order to get around \gls{variability} situations like the one in Listing \ref{lst:staticfinalerror}, where a method's availability is dependent on product configuration, it is possible to use one of two other approaches: Java code reflection, and Java interfaces.

%----------------------------------------------------------

\subsection{Preprocessing with Reflection}
\label{sc:reflection}

Code reflection\footnote{Written with sizeable contributions from Anderson Domingues} is a type of mechanism known as \emph{introspection} where software artifacts are able to analyse and modify themselves during runtime, enabling them to alter their own behaviour based on the software's current conditions. Reflection is one of the oldest programming concepts, dating back to the notion of \emph{self-modifying code} which was present in the earliest assembly language codes. Back then, the idea was basically to keep code inside the data portions of a software and then pull them into the processing portions, essentially changing the execution of the program. Of course, this caused several maintainability problems back then, and occasioned numerous bugs resulting from unexpected runtime modifications. Eventually, the practice was dropped entirely and labeled as bad practice.

The modern notion of reflection is slightly different from self-modifying code. Control mechanisms are set in place in modern programming languages that implement introspection, and the degree to which a program may be modified during runtime is greatly reduced to avoid unexpected behaviour or ``write-only'' code. Among these mechanisms is the restriction that is placed on reflection to be used only to alter runtime behaviour, rather than making permanent modifications to executables and libraries.

While compilation directives are not readily available in Java, it is possible to create a mechanism equal to (and perhaps simpler than) the Variant Factory proposed in \cite{LASER:2015} by using code reflection. The process is similar to that used in C\#: an interface is created to represent a variability point, declaring its public signature, and a Factory class is used to instantiate \gls{variant}s based on that interface. In this implementation, however, we simplify the code related to choosing a \gls{variant} by using reflection to analyse the available \gls{variant}s for a suitable one. This both removes the need to alter the Factory code when a \gls{variant} is created and makes it possible for the system to use a single Factory to realize all \gls{variant}s.

First, the interface for a particular \gls{vpoint} is implemented in its own isolated deployment package, so as to make any dependencies to it restricted and well-controlled. This interface must specify all of the necessary public methods of any \gls{variant} that implements this \gls{vpoint}, and may be declared with default behavior when appropriate (see Listing \ref{lst:vpointinterface}).

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
package ParserVariationPoint;

public interface IParser
{
  default Object parse(Object input)
  {
    return null;
  }
}
\end{minted}
\caption{Example of \gls{vpoint} interface} \label{lst:vpointinterface}
\end{listing}

Next, the \gls{variant}s must be created, also in their own deployment packages, with a facade that implements the \gls{vpoint} interface. Listing \ref{lst:variantfacade} shows an example of this.

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
package ParserX;

import packageName.IParser;

public class ParserX implements IParser
{
  public Object parse(Object input)
  {
    //execute
  }
}
\end{minted}
\caption{Example of \gls{variant} facade} \label{lst:variantfacade}
\end{listing}

At this point, all of the necessary components are in place for us to build a Variant Factory. Listing \ref{lst:variantfactory} presents an example of a Variant Factory implementation. In particular, this implementation uses the notion of ``instructions'' \textcolor{red}{(research pending)} to select which available \gls{variant} best satisfies the request.

%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=true,
               xleftmargin=21pt,
               breaklines=true,
               tabsize=4]{java}
public class VariantFactory
{
  public static Object getInstanceOf(Class c, Configuration f) throws Exception
  {
    //foreach class d that implements c.getClass, if f.contains(d), return d instance;
  }

  public static Object getInstanceOf(Class c, Configuration f, Instructions i)
  {
    //if i == null, getInstanceOf(c,f);

    //foreach class d that implements c.getClass, if f.contains(d) and d follows i, return d instance;
  }
}
\end{minted}
\caption{Example of Variant Factory} \label{lst:variantfactory}
\end{listing}

%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------
%-----------------------------VLAD AQUI----------------------------

%----------------------------------------------------------

\subsection{Interfaces as Variant-Selection Mechanisms}
\label{sc:interfaces}

%----------------------------------------------------------

\subsection{Java Preprocessor Project}
\label{sc:prepros}

%TODO: use UmlElement's "public" as example of use.
%TODO: use UmlAssociation's constructors as example of use.

%----------------------------------------------------------
